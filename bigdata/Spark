Spark实践经验

本文总结 Spark 进行大数据处理、建模的相关经验。

# Spark 分布式计算架构
个人倾向于区分出物理架构以及运行时架构。物理架构是 Spark 集群本身的构成描述；Runtime 架构是 Application 具体执行时的逻辑架构。  

## 物理架构
### Driver Program（Client）
本地启动程序，用于执行 Application。负责创建并持续维护本地基本执行环境 SparkContext（以及 HiveContext 等），并通过我们的配置，访问集群，和 Cluster Manager 以及 执行单元 Executor 进行交互，发布、跟踪我们的任务执行。

### Cluster Manager
集群管理节点；集群常见的部署方式可以是基于 Yarn 或者 Standlone，后者为 Spark 直接负责管理集群资源。管理内容涉及内存、CPU等资源划分以及任务提交队列的管理（比如说一个FIFO队列来管理大家的spark任务提交）。Cluster Manager 和 Driver 通信的时候告知可使用资源，Driver 就可以发送程序到各个 Worker 上执行了。集群底层的分布式文件存储一般使用 hdfs。

### Worker Node
集群执行节点，一个真正的物理节点可以虚拟出多个 Worker。Worker 由 Cluster Manager 管理，任务执行时由 Worker 上创建 Executor 进程实例执行具体代码。Worker 负责管理其创建的 Executor，需要时可以直接 kill 掉。Worker 只对 Cluster manager 负责，不与用户侧 Driver 交互，Driver 之间与相关 Executor 交互。

## Runtime 架构
Driver 上的 SparkContext 和 一众 Worker 上的 Executor 的生命周期是贯穿整个 Application 的

### SparkContext
由 Driver 创建、维护的 SparkContext，用于与整个集群的 Executor 进行直接交互。它可以直接创建 RDD、广播共享变量等等，具体包含了 RDD Graph、DAGScheduler、TaskScheduler等模块。这里面包含着很多重要的元数据，例如各个 RDD 之间的关联信息，所以集群数据丢失时可以精确到partition地自动地高效地恢复丢失数据；以及 Hive 表的 schema 信息等；

### Executor
Executor 是物理节点 Worker 上具体的执行单元，独立的 JVM 进程，又被称为执行具体 Task 的容器 Container。Executor 在初始化时会载入 jar包以及相关依赖。Executor 被 Worker 创建和监控，但执行过程是面向 Driver，直接与 Driver 的 SparkContext 进行交互，接受并执行具体任务（真正工作在一线，执行我们的代码）；相关 RDD 的 cache 也是直接保存在 Executor 的内存中。我们在提交任务的时候可以指定每个 Executor 的计算资源数量（cores）以及内存资源数量（多少gb）；core 的数量决定了 Executor 内部的并发数量，有 n 个 cores 的话就可以并发执行 n 个 Task。


## submit 相关参数
完整的提交指令例子：“spark-submit --class com.minerva.grain.ScoreEngine --num-executors 100 --executor-cores 2 --executor-memory 4g target/scala-2.10/antifraud-minerva-grain_2.10-1.0.jar”
这里注意一个点：**jar 包信息必须放在最后，否则资源配置参数不会生效**


默认参数 --num-executors 10 --executor-cores 1 --executor-memory 2g
推荐环保参数 --num-executors 100 --executor-cores 2 --executor-memory 4g

- num-executor      标识我们申请的执行单元数量
- executor-cores    标识每个执行单元的CPU资源数量，即线程数量（core是虚拟的，不一定能拿到一个完整CPU）
- executor-memory   标识每个执行单元的内存大小（内存太大的话，可能会严重影响集群上其他同学的使用）

从运行上看，一个 executor 一次执行一个 task。一个 stage 上的 task 数量切分越多，则并发程度越高，执行性能可能越好。、
task 的切分是核心点，数据源不同切分标准不同：
- rdd 数据会根据 partition 切分 task
- hdfs 上的数据会根据 InputSplit 切片（合并多个 block）的方式切分 task


（Map-Reduce 常见由 hdfs 的 block 或者 InputSplit 决定 Map task 切分，当然也有好多其他切分方案）




## 实践技巧

- 监控相关 RDD 的具体情况（特别是 partition），可以在合适的地方把它 catch 住，这样后续在任务监控的 Storage 中可以看到它的细节参数，甚至包括物理空间大小
- 注意，写代码时候改换行的地方换行，因为监控日志可以查看具体行的耗时，cache()动作最好单独写一行
- broadcast join 调优，在指定jar之前，加入参数 --conf "spark.sql.autoBroadcastJoinThreshold=100*1024*1024"，这个 threshold 规定了小于100MB（不加参数的话，Spark默认10MB；但最大不能超过2GB）的表join时都使用广播方式，把小的表广播到集群各个节点上去 join
- 一定要注意join时一个数据量过大的key可能会导致严重倾斜
- sort 有可能造成RDD的partition分片极度失衡的情况


