Note of Reinforcement Learning

Reinforcement learning is an attempt to model a complex probability distribution of rewards in relation to a very large number of state-action pairs. This is one reason reinforcement learning is paired with, say, a Markov decision process, a method to sample from a complex distribution to infer its properties.


Recommended Textbook:
Algorithms for Reinforcement Learning, Szepesvari
Recommende wiki:
https://skymind.ai/wiki/deep-reinforcement-learning

The core of Reinforcement Learning - Science of Desicion Making

** Difference with Supervised Learning **
- There is no supervisor, only a reward signal
- Feeeback is delayed, not instantaneous
- Time really matters (sequential, non i.i.d data)
- Agent's actions affect the subsequent data it receives!





# Some basic components and concepts about Reinforcement Learning

## Rewards
A reward is a scalar feedback signal
Indicates how well agent is doing at step t
Reward Hypothesis - All goals can be described by the maximization of expected cumulaive reward.

## Sequential Decision Making
Goal is to select actions to maximise total future reward
Actions may have long term consequences
Reward may be delayed
It may be better to sacrifice immediate reward to gain more long-term reward (you can't be greedy)

## Agent and Environment
take observation, make action, get reward

## History and State

The history is the sequence of observations, actions, rewards
$H_t=A_1,O_1,R_1,...,A_t,O_t,R_t$

What happens next depends on the history.

State is the information used to determine what happens next. Which replaces the history as a summary.
For normally, state is a function of the history: $$S_t = f(H_t)$$
- Environment State (We dont use it), it determines the observation/reward next. But this global state is private and not visible for agent. Agent only see very small part of the enviroment.
- Agent State (We use it), it contains information the agent got.

#### Information State
An information state contains all useful information from the history.
Definition of Markov: $P[S_{t+1}|S_t] = P[S_{t+1}|S_1,S_2,...,S_t]$
Once the state is known, the whole history can be thrown away.

#### Fully Observable Environment
agent directly observes environment state: $$O_t=S_t^{agent}=S_t^{environment}$$
agent state = environment state = information state
Formally, this is a Markov decision process (MDP)

#### Partial Observable Environment
agent state != environment state
Formally, this is a partially observable Markov decision process (POMDP)

#### Policy
A policy suggests which action to take, given a state.
Actually, the goal of reinforcement learning is to discover a good policy.
The optimal policy tells you the optimal action, given any state—but it may not provide the highest reward at the moment.


## Major Components of an RL Agent
Policy: agent's behaviour function, mapping state to action
Value function: how good is each state and/or action
Model: agent's representation of the environment




# Q-learning

## Definition

Implementing the RL(reinforcement-learning) algorithm requires 3 primary steps below:
- infer the best action from the current state
- perform the action
- learn from the results

Q-learning is an approach to solving reinforcement learning whereby you develop an algorithm to approximate the utility function (Q-function).
Once you got Q-function, the policy can be: $Policy(s)=argmax\ Q(s,a)$


## Utility Function (Q-function)

The utility of performing an action $a$ at a state $s$ is written as a function: $Q(s, a)$, which predicts the expected and the total rewards (the immediate reward + rewards gained later by following an optimal policy).

### The commonly used Q-function
$$Q(s, a) = r(s, a) + \gamma \max Q(s', a')$$
It is a recursive function that meature the current utility upon immediate reward and the future rewards.

- $r(s, a)$ is immediate reward of action $a$ for state $s$.
- $\gamma$ is hyperparameter called discount factor. If $\gamma = 0$ then it is just greedy. And higher $\gamma$ considers more about the future. (pretty philosophy that we all put discount for future~)

## Learning process
```
Initialize Q(s,a) arbitrarily
Repeat (for each episode):
	Initialize s
	Repeat (for each step of episode):
		Choose a from s using policy derived from Q (eg. epsilon-greedy)
		Take action a, observe r, s'
		Learn Q(s,a) = Q(s,a) + alpha[r+gamma max Q(s',a') - Q(s,a)]
		s = s'
	until s is terminal
```

The key update rule is: $Q(s,a) = Q(s,a) + \alpha[r+\gamma max_{a'} Q(s',a') - Q(s,a)]$

- $Q(s,a)$ in the right side is the last Q value, which called the predict Q

- the learning target is: $r+\gamma max_{a'} Q(s',a')$, which called the real Q


## Most patterns of reinforcement learning
The pattern is here:
```
	class DecisionPolicy:
	    def select_action(self, current_state):
	       pass
		def update_q(self, state, action, reward, next_state):
   			pass
```

There are 3 steps: infer, do, learn.

- infer; selects the best action $a$, given a state $s$, using the knowledge it has so far.

- do; does this action to find out the reward $r$ as well as the next state $s'$

- learn; improve its understand of the world by using the newly acquired knowledge $(s,a,r,s')$ (update Q-function in our case)


# Sarsa
It is similar to Q-learning, which also uses Q-table to make decision. The difference is about learn step.

## Learning process
```
Initialize Q(s,a) arbitrarily
Repeat (for each episode):
	Initialize s
	Choose a from s using policy derived from Q (eg. epsilon-greedy)
	Repeat (for each step of episode):
		Take action a, observe r, s'
		Choose a' from s' using policy derived from Q (eg. epsilon-greedy)
		Learn Q(s,a) = Q(s,a) + alpha[r + gamma Q(s',a') - Q(s,a)]
		s = s'
		a = a'
	until s is terminal
```

## Key difference from Q-learning

Sarsa does not use max to evaluate real Q, and directly chooses an action for the next state to determine real Q.
Thus the learn (for this step) and do (for next step) are consistent for Sarsa, it has to do the same thing in the next step thus it need on-policy (real time play and learn). For Q-learning, these are separate and it is off-policy (just learn from experiences).

- Sarsa is on-policy & Q-learning is Off-policy
- Sarsa is conservative while Q-learning is brave.

## Sarsa - Lambda
Update Q-values through the whole path(all states) to the terminal. It is usually more efficient.

The key parameter Lambda is a float in $[0,1]$:

- Lambda $0$ is standard Sarsa
- Lambda $1$ gives the whole path the same update
- Lambda $(0,1)$ gives higher strength for the state closer to terminal


In practice, a E(eligibility)-table used to control weights(lambda value of the path), which is the same shape as q-table.

```
Initialize Q(s,a) arbitrarily
Repeat (for each episode):
	Reset E(s,a) = 0, for all s, a
	Initialize s
	Choose a from s using policy derived from Q (eg. epsilon-greedy)
	Repeat (for each step of episode):
		Take action a, observe r, s'
		Choose a' from s' using policy derived from Q (eg. epsilon-greedy)
		error = r + gamma Q(s',a') - Q(s,a)
		E(s, :) = 0
		E(s, a) = 1
		For all s, a:
			Learn Q(s,a) = Q(s,a) + alpha * error * E(s, a)
			Decay E(s, a) = gamma * lambda * E(s, a)
		s = s'
		a = a'
	until s is terminal
```



# DQN - Deep Q-Network
Based 
Replace Q-table by the Neural Network. There are two styles:

 that takes state and action as inputs and output Q-value.



Breaks the dependency of experience
- Experience replay
- Fixed Q-targets




# Markov decision process ( MDPs )

MDPs formally describe an environment for reinforcement learning. 
Where the environment is fully observable, and the current state fully characterises the process.

### Almost all RL problems can be formalised as MDPs:

- Optimal control primarily deals with continuous MDPs
- Partially observable problems can be converted into MDPs
- Bandits are MDPs with one state

It is really fundamental.

### Markov Property
The future is independent of the past given the present. (That is sort of philosophy~)
The current state is a sufficient statistic of the future.
$P[S_{t+1}|S_t] = P[S_{t+1}|S_1,S_2,...,S_t]$

Only needs to care the current state that captures all relevant information from history. Please throw away the past~ 


### State Transition Matrix
For a Markov state $s$ and successor state $s'$, the state transition probability is defined by:
$$P(S_{t+1}=s'|S_t=s)$$

State transition matirx $P$ defines transition probablities from all states $s$ to all successor states $s'$。
(index of row is 'from' and index of  )














# Some notes from Morvan


### Model-Free RL 不理解环境
- Q learning
- Sarsa
- Policy Gradients

### Model-Based RL 理解环境（需要为现实世界建模出一个虚拟世界）
其实就是多了多环境的建模，是的模型具有想象力，可以在虚拟世界中想象整个事态的发展（就像下棋的时候能够一直想象接下来的交手）
- Q learning
- Sarsa
- Policy Gradients


### Policy-Based RL 基于概率
- Policy Gradient

### Value-Based RL 基于价值
- Q learning
-Sarsa


### Monte-Carlo update 回合更新（整个游戏结束了才更新模型）
- Policy Gradient
- Monte-Carlo Learning

### Temporal-Difference update 单步更新（游戏中的每个步骤都迅速更新模型，更有效率）
- Q learning
- Sarsa
- Policy Gradient 

### On-Policy 在线学习（必须本人在场，边玩边学习）
- Sarsa

### Off-Policy 离线学习（看别人玩，根据别人的经验学习）
- Q learning
- Deep Q Network










